{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cebf256d-7cf0-4a76-89d8-df99256c7d3e",
   "metadata": {},
   "source": [
    "# NLP Model\n",
    "\n",
    "This first model is taken from tensorflow hub and is used to embed words into numbers which can then be fed into a machine learning algorithm down the line. Although it is from tensorflow hub we actually use it in keras just for ease of use. The model is called nnlm and is a deep neural network trained on the google english news. More information may be found [here](https://tfhub.dev/google/collections/bert/1). \n",
    "\n",
    "In our particular use case this model is takes a transaction description, converts that into numbers, which is then fed into a classifier to predict what category a transaction is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c3d0c3d-61a1-4816-99b1-d27abb3c432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import tensorflow_text as text  \n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# construct our neural network\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "preprocessor = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1\")\n",
    "encoder_inputs = preprocessor(text_input) # dict with keys: 'input_mask', 'input_type_ids', 'input_word_ids'\n",
    "encoder = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2\",\n",
    "    trainable=True)\n",
    "outputs = encoder(encoder_inputs)\n",
    "pooled_output = outputs[\"pooled_output\"]      # [batch_size, 768].\n",
    "sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 768]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a4de4-fa6f-453d-9f36-5aa3821147c8",
   "metadata": {},
   "source": [
    "Now we can use the model to get our vector for any sentence we would like. In our production version we have more preprocessing to try get rid of the stuff which doesn't mean anything. We use BERT to try and catch semantic similarities between sentences due to the model learning from wikipedia. This means the resultant vector for coffee shop and coffee room will be closer than coffee room and casino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a99fbd5-c2e9-4779-bc3a-5e647fa6a98a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.9999931   0.01890039 -0.99417526 -0.5798835  -0.9798178  -0.475273\n",
      "  -0.7508789  -0.89774495  0.06465519  0.04209847  0.3270732  -0.07334452\n",
      "   0.02483435  0.9999743   0.42316917 -0.90515625  0.70257396  0.15186098\n",
      "  -0.4735886  -0.4672566   0.88990635 -0.06746099 -0.19950417 -0.81234556\n",
      "  -0.99843246 -0.06911208 -0.9996253   0.5582032   0.94886774  0.04736497\n",
      "  -0.04890127 -0.15587012 -0.925856   -0.8101108   0.614291    0.9983587\n",
      "  -0.9991843  -0.00977816  0.97735554 -0.9855164   0.98690736  0.9609173\n",
      "  -0.9610637   0.94345903 -0.9910254  -0.08077802 -0.9433065   0.9964877\n",
      "   0.874688    0.9992928   0.7694149  -0.8159503  -0.06038954  0.35094908\n",
      "   0.7543407   0.88328785 -0.2450968  -0.9747528   0.820621   -0.332429\n",
      "   0.04751019  0.9828596  -0.95191634  0.962528   -0.95750004 -0.99998635\n",
      "  -0.7990878   0.9383162   0.38521895  0.9933143   0.99812734  0.26086992\n",
      "  -0.9505805  -0.08694382  0.9597408  -0.9977386  -0.705001    0.09304172\n",
      "  -0.80885977  0.04232275 -0.12215594  0.07270517 -0.9837704  -0.9942179\n",
      "   0.9976733  -0.95074433 -0.40067476 -0.97732    -0.8726146   0.09806138\n",
      "   0.10925669  0.9217955  -0.9373878   0.71076435  0.5193602   0.7330822\n",
      "  -0.71967965  0.9792316  -0.9996094  -0.6913161  -0.97818345  0.88659346\n",
      "  -0.99965084 -0.4331517  -0.9795837  -0.48231885 -0.9997949  -0.9958964\n",
      "   0.9273489   0.81825733  0.9991184  -0.5006145  -0.88163364  0.98689157\n",
      "  -0.99963963 -0.14669146 -0.90683466 -0.5544076   0.12979276 -0.99955815\n",
      "   0.10615852 -0.99996704 -0.83027136  0.72650534 -0.9984379   0.99013424\n",
      "   0.6538327   0.9649345 ]], shape=(1, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "embedding_model = tf.keras.Model(text_input, pooled_output)\n",
    "sentences = tf.constant([\"tesco superstore sainsburys\"])\n",
    "print(embedding_model(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e65524bc-4c22-416f-84f2-46f4af8a61bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "x = (embedding_model(sentences))\n",
    "\n",
    "vectors_df = pd.DataFrame(np.stack(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b80cc86-4bb1-49f1-a9a8-22443d3a7f8d",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "\n",
    "LightGBM stand for light gradient boosting machines and is a tree based model. It is fast and returns highly accurate results in comparison to other models. The documentation for it can be found [here](https://lightgbm.readthedocs.io/en/latest/) and the wiki page is [here](https://en.wikipedia.org/wiki/LightGBM). We use this as a classification model for our categories when we can not extract the merchants. \n",
    "\n",
    "The model takes an input of 133 (128 from the output from BERT) and also a further 5 features (balance (normalised), day of week ... )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "943140bd-93df-43f7-b12f-12555bcad84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/callumsmyth/PycharmProjects/yapily-ml-models/yapily-ml-models/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.24.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# here I am just import the correct modules and importing a model that was made during testing with the right parameters\n",
    "import os\n",
    "import pickle\n",
    "import lightgbm\n",
    "\n",
    "prefix = '/Users/callumsmyth/PycharmProjects/aws-categorisation-engine/local_test/test_dir/'\n",
    "model_path = os.path.join(prefix, 'model')\n",
    "lg_class_path = os.path.join(model_path, 'lg_class.pkl')\n",
    "\n",
    "with open(os.path.join(lg_class_path), 'rb') as inp:\n",
    "    model = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ed654e8-3a03-4e1d-bc9e-f789758df00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in some more columns so we can get predictions this is just dummy data\n",
    "x_df = pd.DataFrame({\n",
    "    \"xcol1\": [10],\n",
    "    \"xcol2\": [0.5],\n",
    "    \"xcol3\": [0.5],\n",
    "    \"xcol4\": [0.5],\n",
    "    \"xcol5\": [0.1]\n",
    "})\n",
    "predict_df = pd.concat([x_df, vectors_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "057e612c-9c90-4e3a-8a3b-3197a70b9a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HOME' 'OTHER']\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(predict_df)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e53c5-bf6f-4fbb-a50a-a97bbdc1aeaa",
   "metadata": {},
   "source": [
    "So we dont actually go for the predictions and use them directly. We take the probability of each class of income and use and if there is a probability above a certain threshold (0.3) we will use that class. If there is not then we will assign it 'OTHER' and this is what the customer will see as we are not confident in the results.\n",
    "\n",
    "This looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9c585b4c-f338-4cb8-afbb-79a122dc92c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.40637363e-04, 1.01387134e-03, 1.45709923e-04, 4.84323182e-04,\n",
       "        8.93834482e-04, 5.14661064e-03, 2.82819969e-04, 9.14937395e-01,\n",
       "        1.32025780e-02, 1.70162808e-03, 8.84984257e-04, 1.91748153e-03,\n",
       "        1.05822484e-02, 2.21050095e-04, 1.85267757e-04, 4.82595600e-02],\n",
       "       [8.68086675e-03, 2.25433461e-05, 1.26963014e-01, 4.94214977e-02,\n",
       "        7.12566297e-03, 1.18021395e-04, 1.54085304e-02, 3.61664445e-03,\n",
       "        4.07569864e-05, 5.97870242e-04, 4.55172942e-05, 6.10035133e-01,\n",
       "        1.23375729e-04, 1.67146749e-01, 5.99411223e-03, 4.65970364e-03]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(predict_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0439eea3-1ebd-4ebf-9e6d-a580c179dfb5",
   "metadata": {},
   "source": [
    "# Balance Prediction\n",
    "\n",
    "for balance prediction we use a time series forecasting algorithm created by Amazon called DeepAr. This (similarly to BERT) is a neural network and information can be found [here](https://aws.amazon.com/blogs/machine-learning/now-available-in-amazon-sagemaker-deepar-algorithm-for-more-accurate-time-series-forecasting/). Because this is hosted on a docker container by AWS the architecture to use this is slightly different.\n",
    "\n",
    "For categorisation we can 'BYOC' (bring your own container) and use custom code inside a container with a 'train' and 'serve' channel. This keeps all the code nice and compact in one container, and run the 'train' channel to create a new model. And then with the same container run it on 'serve' model and it will serve the model via an API (Flask) to our internal customer.\n",
    "\n",
    "For DeepAr, it is AWS's container and we can't include custom code into it. So we have built a custom container (which we have full control over the input and output of the container) which calls the trained DeepAr model inside of it and then exposes the output back through an API (FastAPI) to our internal clients.\n",
    "\n",
    "Time series models are easier to visualise so we can do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "88af9172-b87a-4597-8142-7edada1ad7a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./yapily-ml-models/lib/python3.8/site-packages (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./yapily-ml-models/lib/python3.8/site-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./yapily-ml-models/lib/python3.8/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in ./yapily-ml-models/lib/python3.8/site-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in ./yapily-ml-models/lib/python3.8/site-packages (from scikit-learn) (1.7.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/Users/callumsmyth/PycharmProjects/yapily-ml-models/yapily-ml-models/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0a1efb6e-c2e2-4e1f-8709-8b2134611d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import json\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "from random import shuffle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pda\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class JsonIfy(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, inference=False, pred_length=None, dynamic_feat=True):\n",
    "        self.inference = inference\n",
    "        self.pred_length = pred_length\n",
    "        self.dynamic_feat = dynamic_feat\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        time_series_jslines = []\n",
    "        for ts in X:\n",
    "            time_series_jslines.append(json.loads(series_to_jsonline(ts, dynamic_feat=self.dynamic_feat,\n",
    "                                                                     pred_length=self.pred_length,\n",
    "                                                                     inference=self.inference)))\n",
    "        return time_series_jslines\n",
    "    \n",
    "def series_to_jsonline(ts, dynamic_feat=None, cat=None, inference=False, pred_length=None):\n",
    "    if inference == False:\n",
    "        return json.dumps(series_to_obj_train(ts, dynamic_feat, cat))\n",
    "    else:\n",
    "        return json.dumps(series_to_obj_inference(ts, dynamic_feat, cat, pred_length))\n",
    "    \n",
    "\n",
    "def series_to_obj_inference(ts, dynamic_feat=None, cat=None, pred_length=None):\n",
    "    target = list(ts['target'])\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": target[:-pred_length]}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        dyn_feat_list = (list(ts['dynamic_features']))\n",
    "        obj[\"dynamic_feat\"] = [dyn_feat_list]\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a6f8fb32-055f-444a-bfef-6ed2348132ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export AWS_DEFAULT_REGION=eu-west-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "082e17dd-6be1-4074-8c41-3573002ba7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deepar-preds.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile deepar-preds.py\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from utils import *\n",
    "endpoint_name = 'balance-reconstruction-staging'\n",
    "\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer()\n",
    ")\n",
    "\n",
    "# setting some constants\n",
    "freq = 'D'\n",
    "prediction_length = 30\n",
    "context_length = 30\n",
    "prefix = 'Scikit-DeepAr-Pipeline/transformed_data/for_testing/'\n",
    "\n",
    "# this is just reading in data from s3 to look at the predictions \n",
    "s3 = boto3.client('s3')\n",
    "actual_data_obj = s3.get_object(Bucket='balance-reconstruction', Key=prefix + 'actual_data')\n",
    "actual_data_obj = actual_data_obj['Body'].read()\n",
    "actual_data = pickle.loads(actual_data_obj)\n",
    "for i in actual_data:\n",
    "    i.columns =[ 'target', 'AccountId', 'dynamic_features']\n",
    "\n",
    "# creating a prediction object\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer())\n",
    "\n",
    "bucket_name = 'balance-reconstruction'\n",
    "prefix = 'Scikit-DeepAr-Pipeline/transformed_data/'\n",
    "client = boto3.client('s3')\n",
    "\n",
    "# nifty function which takes the data in a dataframe (easy to work with) and converts to the format DeepAr expects\n",
    "convert_train_to_json = Pipeline([\n",
    "    ('jsonify', JsonIfy(pred_length=prediction_length, inference=True))\n",
    "])\n",
    "\n",
    "# selecting the previous 30 days\n",
    "prediction_data = []\n",
    "for i in actual_data:\n",
    "    prediction_data.append(i[:-60])\n",
    "\n",
    "# this actual converts our raw data to give to DeepAr and adds some configuration so we can output or 80% confidence interval\n",
    "time_series_training = convert_train_to_json.fit_transform(prediction_data)\n",
    "instances = {\"instances\": time_series_training,  'configuration': {\"num_samples\": 30,\n",
    "                                                                  \"output_types\": [\"quantiles\"],\n",
    "                                                                  \"quantiles\": [\"0.1\", \"0.9\", \"0.5\"]\n",
    "                                                                  }}\n",
    "# do the actual predictions\n",
    "response = predictor.predict(instances)\n",
    "response_data = json.loads(response.decode())\n",
    "\n",
    "\n",
    "# from here onwards its just reformating the data so we can get it into a nice overlayed graph\n",
    "predicted_dataframe = []\n",
    "prediction_times = [x.index[-31] + pd.Timedelta(1, unit=freq) for x in actual_data]\n",
    "\n",
    "prediction_times = []\n",
    "for i in actual_data:\n",
    "    prediction_times.append(i.index[-31] + pd.Timedelta(1, unit=freq))\n",
    "\n",
    "print(f'\\nprediction times are:\\n{prediction_times}\\n')\n",
    "list_of_df = []\n",
    "for k in range(len(prediction_times)):\n",
    "    prediction_index = pd.date_range(\n",
    "        start=prediction_times[k], freq=freq, periods=prediction_length\n",
    "    )\n",
    "    predicted_dataframe.append(\n",
    "        pd.DataFrame(data=response_data[\"predictions\"][k][\"quantiles\"], index=prediction_index)\n",
    "    )\n",
    "\n",
    "for i in actual_data:\n",
    "    i.drop(['dynamic_features', 'AccountId'], inplace=True, axis=1)\n",
    "\n",
    "for k in range(len(prediction_times)):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    actual_data[k].tail(60).plot(label=\"target\")\n",
    "    predicted_dataframe[k][\"0.5\"].plot(label=\"prediction median\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d9302052-1e43-4c89-9ec6-597ba238b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yapily-ml-models",
   "language": "python",
   "name": "yapily-ml-models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
